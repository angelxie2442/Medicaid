{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Created on July 23rd\n",
    "Authors: Yuan-Chi Yang, Angel Xie"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing the data and perform some checks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It consists of all the tweets classified as the 'p' class by the best performing classfiers to date."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./political-tweets-streaming.csv',header = 0, keep_default_na=False,dtype={'tweet_id':str})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check Duplicates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.duplicated(subset = ['tweet_id'], keep=False).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Include 'text_remove_stopwords' column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loadStopWords(FILENAME):\n",
    "    stopword_list = []\n",
    "    infile = open(FILENAME)\n",
    "    for line in infile:\n",
    "        stopword_list.append(line.strip())\n",
    "    print(len(stopword_list))\n",
    "    return stopword_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hashtag_funx(hashtag):\n",
    "    #clean the symbols and numbers\n",
    "    hashtag=re.sub(r'([^A-Za-z])',' ',hashtag)#clear symbols\n",
    "    hashtag = re.sub(r'([A-Z][a-z]{1,})',lambda x: ' '+x.group(),hashtag)\n",
    "    hashtag = re.sub(r'([A-Z]{2,})',lambda x: ' '+x.group(),hashtag)\n",
    "    hashtag=re.sub(r'^([A-Z]{1,1}\\s+)|(\\s+[A-Z]{1,1}\\s+)|(\\s+[A-Z]{1,1})$',' ',hashtag)\n",
    "    hashtag=re.sub(r'\\s{2,}',' ',hashtag)\n",
    "    return hashtag\n",
    "\n",
    "def processing_text_remove_stopwords(tweet_text,stop_words):\n",
    "    # replace '&amp' with 'and'\n",
    "    tweet_text = re.sub(r'&amp;', \"and\", tweet_text)\n",
    "    # remove url\n",
    "    tweet_text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', tweet_text)\n",
    "    # remove username\n",
    "    tweet_text = re.sub('(@[\\S]+)', '', tweet_text) #tweet_text = re.sub('(@[A-Za-z0-9\\_]+)', '', tweet_text)\n",
    "    # extract the meaningful terms in the hashtag\n",
    "    tweet_text=re.sub('(#[\\S]+)',lambda x: hashtag_funx(x.group()),tweet_text)\n",
    "    # remove non-english charactor\n",
    "    tweet_text = re.sub(\"[^a-zA-Z_-]\", \" \", tweet_text)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    tweet_text = re.sub(r'\\s{2,}', \" \", tweet_text)\n",
    "    tweet_text_save=tweet_text\n",
    "    tweet_text = [t for t in tweet_text.split() if (not t in stop_words and len(t)>=3)]\n",
    "    if len(tweet_text)==0 and \"medicaid for all\" in tweet_text_save:\n",
    "        tweet_text=[\"medicaid for all\"]\n",
    "    return ' '.join(tweet_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopwords = set(loadStopWords('./stopwords.txt'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['text_remove_stopwords'] = df['unprocessed_text'].apply(lambda x: processing_text_remove_stopwords(x,stopwords))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract and Remove Near-duplicates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "def remove_duplicates(df,col_title):\n",
    "    indices=[]\n",
    "    df=df.sort_values(by=[col_title],ascending=True)\n",
    "    str_prev=df.iloc[0][col_title]\n",
    "    for index in range(len(df[col_title])):\n",
    "        curr=index\n",
    "        str_curr=df.iloc[curr][col_title]\n",
    "        if((abs(len(str_curr)-len(str_prev))<max(len(str_prev),len(str_curr))/8) and fuzz.token_set_ratio(str_curr,str_prev)>97):\n",
    "            if(curr>0):\n",
    "                indices.append(curr)\n",
    "        str_prev=str_curr\n",
    "    to_b_removed=[df.index[ind] for ind in indices]\n",
    "    df_removed=[df.iloc[indd,] for indd in indices]\n",
    "    pd.DataFrame(df_removed).to_csv(\"nearduplicates.csv\")\n",
    "    df=df.drop(to_b_removed)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find the most frequent words in text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unigrams = ' '.join(df['text_remove_stopwords'].to_list()).split()\n",
    "bigrams = []\n",
    "for tweet in df['text_remove_stopwords'].to_list():\n",
    "    bigrams += list(nltk.bigrams(tweet.split()))\n",
    "\n",
    "trigrams = []\n",
    "for tweet in df['text_remove_stopwords'].to_list():\n",
    "    trigrams += list(nltk.trigrams(tweet.split()))\n",
    "uni_fd = nltk.FreqDist(unigrams)\n",
    "big_fd = nltk.FreqDist(bigrams)\n",
    "trig_fd = nltk.FreqDist(trigrams)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Find the number of tweets containing each of the top 40 terms (uni,bi,trigrams)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def wordInNumTweets(s):\n",
    "    pattern = rf'(^|[^a-zA-Z]){s}([^a-zA-Z]|$)' #rf is for using a variable inside\n",
    "    count=0\n",
    "    for i in range(len(df['text_remove_stopwords'])):\n",
    "         if re.search(pattern,df['text_remove_stopwords'].iloc[i]) is not None :\n",
    "            count=count+1\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Output results to the file 'ubt40.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num = 40\n",
    "#uni\n",
    "fd = uni_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "df_term1=[]\n",
    "df_count1=[]\n",
    "df_numtweets1=[]\n",
    "for i in range(0,num):\n",
    "    df_term1.append(fd_list[i][0])\n",
    "    df_count1.append(fd_list[i][1])\n",
    "    df_numtweets1.append(wordInNumTweets(fd_list[i][0]))\n",
    "#bi\n",
    "fd = big_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "df_term2=[]\n",
    "df_count2=[]\n",
    "df_numtweets2=[]\n",
    "for i in range(0,num):\n",
    "    x, y= fd_list[i][0]\n",
    "    term = x + ' '+ y\n",
    "    df_term2.append(term)\n",
    "    df_count2.append(fd_list[i][1])\n",
    "    df_numtweets2.append(wordInNumTweets(term))\n",
    "#tri\n",
    "fd = trig_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "df_term3=[]\n",
    "df_count3=[]\n",
    "df_numtweets3=[]\n",
    "for i in range(0,num):\n",
    "    x, y, z= fd_list[i][0]\n",
    "    term = x + ' '+ y + ' ' + z\n",
    "    df_term3.append(term)\n",
    "    df_count3.append(fd_list[i][1])\n",
    "    df_numtweets3.append(wordInNumTweets(term))\n",
    "df_ubt={'uni_term':df_term1,'uni_count':df_count1,'uni_numtw':df_numtweets1,'bi_term':df_term2,'bi_count':df_count2,'bi_numtw':df_numtweets2,'tri_term':df_term3,'tri_count':df_count3,'tri_numtw':df_numtweets3}\n",
    "df_ubt=pd.DataFrame(df_ubt)\n",
    "df_ubt.to_csv('ubt40.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add sentiment scores to all tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def addpolarityscores(text):\n",
    "    t=TextBlob(text)\n",
    "    return t.sentiment.polarity\n",
    "def addsubjectivityscores(text):\n",
    "    t=TextBlob(text)\n",
    "    return t.sentiment.subjectivity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['polarity'] = df['text_remove_stopwords'].apply(lambda x:addpolarityscores(x))\n",
    "df['subjectivity'] = df['text_remove_stopwords'].apply(lambda x:addsubjectivityscores(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add label of 1/0 based on the existence/absence of each interesting term-the columns are named after the terms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def highfreqword(text,terms):\n",
    "    exist=0\n",
    "    for j in range(len(terms)):\n",
    "        pattern = rf'(^|[^a-zA-Z]){terms[j]}([^a-zA-Z]|$)' #rf is for using a variable inside\n",
    "        if(re.search(pattern,text)!=None):\n",
    "            return 1\n",
    "    return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "term_ls=[['cut social security','cutting social security','cuts social security','social security cuts'],['mental health'],['middle class'],['affordable care act','aca'],['tax cut','tax cuts'],['food stamps'],['low income'],['planned parenthood'],['minimum wage'],['illegal immigrants']]\n",
    "for i in range(10):\n",
    "    label_name=(term_ls[i])[0]\n",
    "    df[label_name] = df['text_remove_stopwords'].apply(lambda x:highfreqword(x,term_ls[i]))\n",
    "df.to_csv('df_labels_scores.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the new dataframe 'df_labels_scores.csv' with added labels and scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "df = pd.read_csv('./df_labels_scores.csv',header = 0, keep_default_na=False,dtype={'tweet_id':str})\n",
    "### reformat date\n",
    "df['yr_month'] = df['time'].apply(lambda x:x[:7])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import PercentFormatter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Polarity Distribution Plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure,axes=plt.subplots(3,4,tight_layout=True,sharey=\"all\",figsize=(15,15))\n",
    "titles=['affordable care act','food stamps','cut social security','tax cut','middle class','low income','mental health','minimum wage','illegal immigrants','planned parenthood']\n",
    "binls=[-1,-0.75,-0.5,-0.25,0,0.25,0.5,0.75,1]\n",
    "axes=axes.ravel()\n",
    "for ax,title in zip(axes,titles):\n",
    "    subset=df[df[title]==1]\n",
    "    ax.hist(subset['polarity'],weights=np.ones(len(subset))/len(subset),bins=binls)\n",
    "    ax.axvline(subset['polarity'].median(),linestyle='dashed',color='r',label=\"median \"+str(round(np.median(subset['polarity']),3)))\n",
    "    ax.axvline(subset['polarity'].mean(),linestyle='dashed',color='y',label=\"mean \"+str(round(np.mean(subset['polarity']),3)))\n",
    "    ax.legend(loc='upper right')\n",
    "    title1='Polarity:'+title\n",
    "    ax.set_title(title1)\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Subjectivity Distribution Plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure,axes=plt.subplots(3,4,tight_layout=True,sharey=\"all\",figsize=(15,15))\n",
    "titles=['affordable care act','food stamps','cut social security','tax cut','middle class','low income','mental health','minimum wage','illegal immigrants','planned parenthood']\n",
    "binls=[0,0.125,0.25,0.375,0.5,0.625,0.75,0.875,1]\n",
    "axes=axes.ravel()\n",
    "for ax,title in zip(axes,titles):\n",
    "    subset=df[df[title]==1]\n",
    "    ax.hist(subset['subjectivity'],weights=np.ones(len(subset))/len(subset),bins=binls)\n",
    "    ax.axvline(subset['subjectivity'].median(),linestyle='dashed',color='r',label=\"median \"+str(round(np.median(subset['subjectivity']),3)))\n",
    "    ax.axvline(subset['subjectivity'].mean(),linestyle='dashed',color='y',label=\"mean \"+str(round(np.mean(subset['subjectivity']),3)))\n",
    "    ax.legend(loc='upper right')\n",
    "    title1='Subjectivity:'+title\n",
    "    ax.set_title(title1)\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate a Table with:\n",
    "##### Term_Name,Year_Month,Mean_Scores,Monthly Standard Deviations, and Monthly Number of Occurrences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "titles=['affordable care act','food stamps','cut social security','tax cut','middle class','low income','mental health','minimum wage','illegal immigrants','planned parenthood']\n",
    "terms=[]\n",
    "times=[]\n",
    "occs=[]\n",
    "pol_means=[]\n",
    "subj_means=[]\n",
    "pol_err=[]\n",
    "subj_err=[]\n",
    "for title in titles:\n",
    "    df1=df[df[title]==1]\n",
    "    x1=list(df1.groupby(['yr_month']).groups.keys())\n",
    "    for xx1 in x1:\n",
    "        terms.append(title)\n",
    "        times.append(xx1)\n",
    "        subset=df1[df1['yr_month']==xx1]\n",
    "        subj_err.append(np.std(subset['subjectivity']))\n",
    "        subj_means.append((subset['subjectivity']).mean())\n",
    "        pol_err.append(np.std(subset['polarity']))\n",
    "        pol_means.append((subset['polarity']).mean())\n",
    "        occs.append(len(subset))\n",
    "table={'term':terms,'yr_month':times,'num_of_occurrences':occs,'polarity_mean':pol_means,'polarity_st_dev':pol_err,'subjectivity_mean':subj_means,'subjectivity_st_dev':subj_err}\n",
    "table=pd.DataFrame(table)\n",
    "table.to_csv('times_series_table.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Time Series plots of Polarity Trend for all ten terms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plt.errorbar(x1,y1,yerr=se1,marker='s',mfc='red',mec='blue',ms=3,mew=5)\n",
    "figure,axes=plt.subplots(3,4,tight_layout=True,sharey=\"all\",figsize=(20,20))\n",
    "titles=['affordable care act','food stamps','cut social security','tax cut','middle class','low income','mental health','minimum wage','illegal immigrants','planned parenthood']\n",
    "axes=axes.ravel()\n",
    "for ax,title in zip(axes,titles):\n",
    "    subset=table[table['term']==title]\n",
    "    x_labels=subset['yr_month'].apply(lambda x:(str(x)[2:4]+'\\n'+str(x)[5:7]))\n",
    "    ax.errorbar(x_labels,subset['polarity_mean'],yerr=subset['polarity_st_dev'],marker='s',ms=7)\n",
    "    title1='Polarity Trend of:'+title\n",
    "    ax.set_title(title1)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Time Series plots of Subjectivity Trend for all ten terms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plt.errorbar(x1,y1,yerr=se1,marker='s',mfc='red',mec='blue',ms=3,mew=5)\n",
    "figure,axes=plt.subplots(3,4,tight_layout=True,sharey=\"all\",figsize=(20,20))\n",
    "titles=['affordable care act','food stamps','cut social security','tax cut','middle class','low income','mental health','minimum wage','illegal immigrants','planned parenthood']\n",
    "axes=axes.ravel()\n",
    "for ax,title in zip(axes,titles):\n",
    "    subset=table[table['term']==title]\n",
    "    x_labels=subset['yr_month'].apply(lambda x:(str(x)[2:4]+'\\n'+str(x)[5:7]))\n",
    "    ax.errorbar(x_labels,subset['subjectivity_mean'],yerr=subset['subjectivity_st_dev'],marker='s',ms=7)\n",
    "    title1='Subjectivity Trend of:'+title\n",
    "    ax.set_title(title1)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Graph Monthly Occurrences for all ten terms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plt.errorbar(x1,y1,yerr=se1,marker='s',mfc='red',mec='blue',ms=3,mew=5)\n",
    "figure,axes=plt.subplots(3,4,tight_layout=True,sharey=\"all\",figsize=(20,20))\n",
    "titles=['affordable care act','food stamps','cut social security','tax cut','middle class','low income','mental health','minimum wage','illegal immigrants','planned parenthood']\n",
    "axes=axes.ravel()\n",
    "for ax,title in zip(axes,titles):\n",
    "    subset=table[table['term']==title]\n",
    "    x_labels=subset['yr_month'].apply(lambda x:(str(x)[2:4]+'\\n'+str(x)[5:7]))\n",
    "    ax.plot(x_labels,subset['num_of_occurrences'],marker='s',ms=7)\n",
    "    title1='Occurrences of:'+title\n",
    "    ax.set_title(title1)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}