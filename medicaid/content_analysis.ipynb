{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on 4/2/2020\n",
    "Author: Yuan-Chi Yang\n",
    "\n",
    "Objective: template for content analysis on political feedback, please feel free to modify and play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data and perform some checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is from \"C:\\Users\\yyang60\\PostDoct-Emory\\medicaid-project\\medicaid-classifier\\labeling-data\\whole_dataset\\BERT_20200228\\political-tweets-streaming.csv\"\n",
    "\n",
    "It consists of all the tweets classified as the 'p' class by the best performing classfiers to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./political-tweets-streaming.csv',header = 0, keep_default_na=False,dtype={'tweet_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "383969"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['tweet_id', 'unprocessed_text', 'class'], dtype='object')"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Duplicates\n",
    "No Duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated(subset = ['tweet_id'], keep=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include 'text_remove_stopwords' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords(FILENAME):\n",
    "    stopword_list = []\n",
    "    infile = open(FILENAME)\n",
    "    for line in infile:\n",
    "        stopword_list.append(line.strip())\n",
    "    print(len(stopword_list))\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_text_remove_stopwords(tweet_text,stop_words):\n",
    "    tweet_text = re.sub(r'&amp;', \"and\", tweet_text)\n",
    "    tweet_text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',\n",
    "                        tweet_text)\n",
    "    #tweet_text = re.sub('(@[A-Za-z0-9\\_]+)', '', tweet_text)\n",
    "    tweet_text = re.sub('(@[\\S]+)', '', tweet_text)#added Angel\n",
    "    ####the following section separate words following each hashtag\n",
    "    list_hashtag1=re.findall('(#[\\S]+)',tweet_text)\n",
    "    list_hashtag2=[]\n",
    "    for p in list_hashtag1:\n",
    "        p=re.sub('(#)',' ',p)#clear hashtag symbols\n",
    "        p1=re.findall('([A-Z]{2,})',p)#uppercase abbrevs\n",
    "        for p2 in p1:\n",
    "            list_hashtag2.append(p2)#add abbrevs\n",
    "        p=re.sub('([A-Z]{2,})',' ',p)#clear uppercase abbrevs\n",
    "        p1=re.findall('([A-Z][a-z]{1,})',p)#words start uppercase letter\n",
    "        for p2 in p1:\n",
    "            list_hashtag2.append(p2)\n",
    "        p=re.sub('([A-Z][a-z]{1,})',' ',p)#clear words start with uppercase letter\n",
    "        p=re.sub('([A-Z])',' ',p)#clear single uppercase letters\n",
    "        p=re.sub('([^A-Za-z])',' ',p)#clear symbols\n",
    "        p1=p.split()#find leftover words\n",
    "        for p2 in p1:\n",
    "            list_hashtag2.append(p2)#add leftover words\n",
    "    tweet_text=re.sub('(#[\\S]+)','',tweet_text)\n",
    "    \n",
    "    tweet_text = re.sub(\"[^a-zA-Z_-]\", \" \", tweet_text)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    tweet_text = re.sub(r'\\s{2,}', \" \", tweet_text)\n",
    "    list_hashtag= [h for h in list_hashtag2 if (not h in stop_words and len(h)>1)]\n",
    "    tweet_text = [t for t in tweet_text.split() if (not t in stop_words and len(t)>1)]\n",
    "    tweet_text.extend(list_hashtag)\n",
    "    return ' '.join(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(loadStopWords('./stopwords.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_remove_stopwords'] = df['unprocessed_text'].apply(lambda x: processing_text_remove_stopwords(x,stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most frequent words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = ' '.join(df['text_remove_stopwords'].to_list()).split()\n",
    "bigrams = []\n",
    "for tweet in df['text_remove_stopwords'].to_list():\n",
    "    bigrams += list(nltk.bigrams(tweet.split()))\n",
    "    \n",
    "trigrams = []\n",
    "for tweet in df['text_remove_stopwords'].to_list():\n",
    "    trigrams += list(nltk.trigrams(tweet.split()))\n",
    "uni_fd = nltk.FreqDist(unigrams)\n",
    "big_fd = nltk.FreqDist(bigrams)\n",
    "trig_fd = nltk.FreqDist(trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordInNumTweets(s):\n",
    "    pattern = rf'(^|[^a-zA-Z]){s}([^a-zA-Z]|$)' #rf is for using a variable inside\n",
    "    count=0\n",
    "    for i in range(len(df['text_remove_stopwords'])):\n",
    "         if re.search(pattern,df['text_remove_stopwords'].iloc[i]) is not None :\n",
    "            count=count+1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-182-3998600fbf1c>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-182-3998600fbf1c>\"\u001B[1;36m, line \u001B[1;32m11\u001B[0m\n\u001B[1;33m    df_uni['num_of_tweets']=\u001B[0m\n\u001B[1;37m                            ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "num = 40\n",
    "print('unigrams:\\n')\n",
    "fd = uni_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "df_ubt=[]\n",
    "for i in range(0,num):\n",
    "    print(fd_list[i][0], fd_list[i][1])\n",
    "    df_ubt['uni_term']=fd_list[i][0]\n",
    "    df_ubt['uni_count']=fd_list[i][1]\n",
    "    df_ubt['uni_num_of_tweets']=wordInNumTweets(fd_list[i][0])\n",
    "\n",
    "print('--------------------------------------------------------\\n')\n",
    "    \n",
    "print('bigrams:\\n')\n",
    "fd = big_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "for i in range(0,num):\n",
    "    x, y= fd_list[i][0]\n",
    "    term = x + ' '+ y\n",
    "    print(term, fd_list[i][1])\n",
    "    df_ubt['bi_term']=term\n",
    "    df_ubt['bi_count']=fd_list[i][1]\n",
    "    df_ubt['bi_num_of_tweets']=wordInNumTweets(term)\n",
    "\n",
    "print('--------------------------------------------------------\\n')\n",
    "print('trigrams:\\n')\n",
    "\n",
    "fd = trig_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "for i in range(0,num):\n",
    "    x, y, z= fd_list[i][0]\n",
    "    term = x + ' '+ y + ' ' + z\n",
    "    print(term, fd_list[i][1])\n",
    "    df_ubt['tri_term']=term\n",
    "    df_ubt['tri_count']=fd_list[i][1]\n",
    "    df_ubt['tri_num_of_tweets']=wordInNumTweets(term)\n",
    "\n",
    "df_ubt.to_csv('uni_bi_tri.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the interesting terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highfreqword(text,term):\n",
    "    pattern = rf'(^|[^a-zA-Z]){term}([^a-zA-Z]|$)' #rf is for using a variable inside\n",
    "    #pattern = rf'{term}'\n",
    "    if(re.search(pattern,text)!=None):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bigrams_ls=['social security','mental health','healthcare','insurance','food stamps','health care','health insurance','private insurance','cuts social','take away','work requirements','trump budget','pre-existing conditions','insurance companies','expansion','budget cuts']\n",
    "for i in range(16):\n",
    "    coln=bigrams_ls[i]\n",
    "    df[coln] = df['text_remove_stopwords'].apply(lambda x:highfreqword(x,coln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "3382"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp=df[df['middle class'].apply(lambda x:x==1)]\n",
    "len(df_temp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-132-7e03087d1343>:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_temp1=df_temp[df[bigrams_ls[i]].apply(lambda x:x==1)]\n",
      "<ipython-input-132-7e03087d1343>:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_temp1=df_temp[df['healthcare'].apply(lambda x:x==1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862\n",
      "11\n",
      "458\n",
      "385\n",
      "125\n",
      "157\n",
      "100\n",
      "47\n",
      "108\n",
      "75\n",
      "8\n",
      "31\n",
      "43\n",
      "18\n",
      "144\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    df_temp1=df_temp[df[bigrams_ls[i]].apply(lambda x:x==1)]\n",
    "    print(len(df_temp1))\n",
    "df_temp1=df_temp[df['healthcare'].apply(lambda x:x==1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "#term2 = 'trump budget'\n",
    "#pattern = rf'(^|[^a-zA-Z]){term2}([^a-zA-Z]|$)' #rf is for using a variable inside\n",
    "#df_temp2 = df[df['text_remove_stopwords'].apply(lambda x: re.search(pattern,x)!=None)]\n",
    "#print('# of tweets:',len(df_temp2))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of duplicated tweets: 0\n"
     ]
    }
   ],
   "source": [
    "#df_temp12 =df_temp1.append(df_temp2)\n",
    "#print('# of duplicated tweets:',df_temp12.duplicated(subset = ['tweet_id'], keep=False).sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "num = 30\n",
    "temp = df_temp1.sample(n=num)\n",
    "#for i in range(0,num):\n",
    "    #print(temp['unprocessed_text'].iloc[i],'\\n\\n')\n",
    "    #print(temp['text_remove_stopwords'].iloc[i],'\\n\\n')\n",
    "    #print('---------------------------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp.to_csv('ss_pec_sample.csv')\n",
    "#temp.to_csv('mh_e_sample.csv')\n",
    "temp.to_csv('mc_h_sample.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}