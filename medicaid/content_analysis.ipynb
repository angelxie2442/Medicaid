{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on 4/2/2020\n",
    "Author: Yuan-Chi Yang\n",
    "\n",
    "Objective: template for content analysis on political feedback, please feel free to modify and play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data and perform some checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is from \"C:\\Users\\yyang60\\PostDoct-Emory\\medicaid-project\\medicaid-classifier\\labeling-data\\whole_dataset\\BERT_20200228\\political-tweets-streaming.csv\"\n",
    "\n",
    "It consists of all the tweets classified as the 'p' class by the best performing classfiers to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./political-tweets-streaming.csv',header = 0, keep_default_na=False,dtype={'tweet_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "383969"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['tweet_id', 'unprocessed_text', 'class'], dtype='object')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Duplicates\n",
    "No Duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated(subset = ['tweet_id'], keep=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include 'text_remove_stopwords' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords(FILENAME):\n",
    "    stopword_list = []\n",
    "    infile = open(FILENAME)\n",
    "    for line in infile:\n",
    "        stopword_list.append(line.strip())\n",
    "    print(len(stopword_list))\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_text_remove_stopwords(tweet_text,stop_words):\n",
    "    tweet_text = re.sub(r'&amp;', \"and\", tweet_text)\n",
    "    tweet_text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',\n",
    "                        tweet_text)\n",
    "    #tweet_text = re.sub('(@[A-Za-z0-9\\_]+)', '', tweet_text)\n",
    "    tweet_text = re.sub('(@[\\S]+)', '', tweet_text)#added Angel\n",
    "    ####the following section separate words following each hashtag\n",
    "    list_hashtag1=re.findall('(#[\\S]+)',tweet_text)\n",
    "    list_hashtag2=[]\n",
    "    for p in list_hashtag1:\n",
    "        p=re.sub('(#)','',p)#string\n",
    "        p1=re.findall('([A-Z]{2,})',p)#list\n",
    "        for p2 in p1:\n",
    "            list_hashtag2.append(p2)\n",
    "        p=re.sub('([A-Z]{2,})',' ',p)\n",
    "        p1=re.findall('([A-Z][a-z]{1,})',p)\n",
    "        for p2 in p1:\n",
    "            list_hashtag2.append(p2)\n",
    "        p=re.sub('([A-Z][a-z]{1,})',' ',p)\n",
    "        p=re.sub('([A-Z])',' ',p)\n",
    "        p=re.sub('([^A-Za-z])',' ',p)\n",
    "        p1=p.split()\n",
    "        for p2 in p1:\n",
    "            list_hashtag2.append(p2)\n",
    "    tweet_text=re.sub('(#[\\S]+)','',tweet_text)\n",
    "    \n",
    "    tweet_text = re.sub(\"[^a-zA-Z_-]\", \" \", tweet_text)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    tweet_text = re.sub(r'\\s{2,}', \" \", tweet_text)\n",
    "    list_hashtag= [h for h in list_hashtag2 if (not h in stop_words and len(h)>3)]\n",
    "    tweet_text = [t for t in tweet_text.split() if (not t in stop_words and len(t)>3)]\n",
    "    tweet_text.extend(list_hashtag)\n",
    "    return ' '.join(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(loadStopWords('./stopwords.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_remove_stopwords'] = df['unprocessed_text'].apply(lambda x: processing_text_remove_stopwords(x,stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most frequent words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = ' '.join(df['text_remove_stopwords'].to_list()).split()\n",
    "bigrams = []\n",
    "for tweet in df['text_remove_stopwords'].to_list():\n",
    "    bigrams += list(nltk.bigrams(tweet.split()))\n",
    "    \n",
    "trigrams = []\n",
    "for tweet in df['text_remove_stopwords'].to_list():\n",
    "    trigrams += list(nltk.trigrams(tweet.split()))\n",
    "uni_fd = nltk.FreqDist(unigrams)\n",
    "big_fd = nltk.FreqDist(bigrams)\n",
    "trig_fd = nltk.FreqDist(trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams:\n",
      "\n",
      "people 66954\n",
      "social 64162\n",
      "security 59281\n",
      "care 51287\n",
      "health 41356\n",
      "would 40135\n",
      "insurance 39921\n",
      "trump 39077\n",
      "healthcare 38641\n",
      "expansion 38198\n",
      "like 35822\n",
      "state 29422\n",
      "cuts 28059\n",
      "need 26424\n",
      "want 26282\n",
      "food 23709\n",
      "work 23698\n",
      "programs 23589\n",
      "money 22908\n",
      "states 21239\n",
      "free 20572\n",
      "know 19507\n",
      "also 19277\n",
      "take 19129\n",
      "government 19113\n",
      "poor 19012\n",
      "public 18829\n",
      "many 18774\n",
      "stamps 18513\n",
      "budget 18252\n",
      "--------------------------------------------------------\n",
      "\n",
      "bigrams:\n",
      "\n",
      "social security 56353\n",
      "food stamps 18234\n",
      "health care 16445\n",
      "health insurance 8658\n",
      "private insurance 5960\n",
      "cuts social 4551\n",
      "take away 4155\n",
      "work requirements 3638\n",
      "middle class 3531\n",
      "trump budget 3182\n",
      "Medicare Medicaid 3103\n",
      "mental health 3037\n",
      "budget cuts 3009\n",
      "pre-existing conditions 2964\n",
      "insurance companies 2753\n",
      "poor people 2727\n",
      "public schools 2648\n",
      "public option 2572\n",
      "programs like 2491\n",
      "social programs 2358\n",
      "minimum wage 2324\n",
      "Social Security 2272\n",
      "illegal immigrants 2250\n",
      "single payer 2234\n",
      "affordable care 2144\n",
      "medical care 2128\n",
      "billion billion 2044\n",
      "planned parenthood 2002\n",
      "rural hospitals 1961\n",
      "socialist programs 1944\n",
      "--------------------------------------------------------\n",
      "\n",
      "trigrams:\n",
      "\n",
      "cuts social security 4383\n",
      "cutting social security 1805\n",
      "welfare food stamps 1738\n",
      "billion social security 1732\n",
      "like social security 1466\n",
      "trillion billion billion 1304\n",
      "billion billion social 1173\n",
      "social security cuts 1139\n",
      "social security food 1107\n",
      "social security public 1086\n",
      "food stamps housing 1059\n",
      "social security budget 1031\n",
      "security food stamps 965\n",
      "want social security 839\n",
      "away social security 820\n",
      "social security disability 817\n",
      "healthcare social security 800\n",
      "cuts trillion billion 783\n",
      "wants social security 770\n",
      "wouldn social security 762\n",
      "social security snap 751\n",
      "trump budget cuts 743\n",
      "Social Security Medicare 729\n",
      "social security benefits 717\n",
      "protect social security 707\n",
      "trump said wouldn 705\n",
      "private health insurance 700\n",
      "universal health care 694\n",
      "food stamps welfare 684\n",
      "social security billion 674\n"
     ]
    }
   ],
   "source": [
    "num = 30\n",
    "\n",
    "print('unigrams:\\n')\n",
    "fd = uni_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "for i in range(0,num):\n",
    "    print(fd_list[i][0], fd_list[i][1])\n",
    "    \n",
    "print('--------------------------------------------------------\\n')\n",
    "    \n",
    "print('bigrams:\\n')\n",
    "fd = big_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "for i in range(0,num):\n",
    "    x, y= fd_list[i][0]\n",
    "    term = x + ' '+ y\n",
    "    print(term, fd_list[i][1])\n",
    "\n",
    "print('--------------------------------------------------------\\n')\n",
    "print('trigrams:\\n')\n",
    "\n",
    "fd = trig_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "for i in range(0,num):\n",
    "    x, y, z= fd_list[i][0]\n",
    "    term = x + ' '+ y + ' ' + z\n",
    "    print(term, fd_list[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the interesting terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tweets: 54964\n"
     ]
    }
   ],
   "source": [
    "term = 'social security'\n",
    "pattern = rf'(^|[^a-zA-Z]){term}([^a-zA-Z]|$)' #rf is for using a variable inside\n",
    "df_temp = df[df['text_remove_stopwords'].apply(lambda x: re.search(pattern,x)!=None)]\n",
    "print('# of tweets:',len(df_temp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a shame there's no tangible items like tanks or planes that can be used to demonstrate the costs of Social Security, Medicare, and Medicaid. \n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "@HandicapperBill @ewarren 20 trillion in accumulated deficits;\n",
      "100 trillion + in unfunded mandates for Social Security, Medicare and Medicaid.\n",
      "Trillions more in unfunded pensions that the feds have guaranteed.\n",
      "It’ll take just one failed bond auction...\n",
      "So, if a private entity, bankrupt long ago. \n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "@LadyRedWave @jimdwrench @larryelder Illegal immigrants with Illegal Documents are still Illegal. Only US Citizen's have Citizen's Rights. They Cannot Vote in US Elections.\n",
      "\n",
      "Cannot Have Free:\n",
      "1. Food Stamps\n",
      "2. Social Security\n",
      "3. Welfare\n",
      "4. Healthcare\n",
      "5. Medicare\n",
      "6. Medicaid \n",
      "7. Housing\n",
      "8. Cell Phones\n",
      "9. Social Scv. \n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Tell Congress: We must protect Medicaid, Medicare and Social Security! https://t.co/aeOMJ0UbH1 \n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "@ritz245552 @flautista21 @CCrook16 @cameron_kasky No it’s a result of Medicare Medicaid and social security \n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "term = 'social security'\n",
    "pattern = rf'(^|[^a-zA-Z]){term}([^a-zA-Z]|$)' #rf is for using a variable inside\n",
    "df_temp = df[df['text_remove_stopwords'].apply(lambda x: re.search(pattern,x)!=None)]\n",
    "print('# of tweets:',len(df_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num = 5\n",
    "temp = df_temp.sample(n=num)\n",
    "for i in range(0,num):\n",
    "    print(temp['unprocessed_text'].iloc[i],'\\n\\n')\n",
    "    #print(temp['text_remove_stopwords'].iloc[i],'\\n\\n')\n",
    "    print('---------------------------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}