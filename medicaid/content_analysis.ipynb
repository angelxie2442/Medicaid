{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on 4/2/2020\n",
    "Author: Yuan-Chi Yang\n",
    "\n",
    "Objective: template for content analysis on political feedback, please feel free to modify and play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data and perform some checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is from \"C:\\Users\\yyang60\\PostDoct-Emory\\medicaid-project\\medicaid-classifier\\labeling-data\\whole_dataset\\BERT_20200228\\political-tweets-streaming.csv\"\n",
    "\n",
    "It consists of all the tweets classified as the 'p' class by the best performing classfiers to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./political-tweets-streaming.csv',header = 0, keep_default_na=False,dtype={'tweet_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "383969"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['tweet_id', 'unprocessed_text', 'class'], dtype='object')"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Duplicates\n",
    "No Duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated(subset = ['tweet_id'], keep=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include 'text_remove_stopwords' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords(FILENAME):\n",
    "    stopword_list = []\n",
    "    infile = open(FILENAME)\n",
    "    for line in infile:\n",
    "        stopword_list.append(line.strip())\n",
    "    print(len(stopword_list))\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_text_remove_stopwords(tweet_text,stop_words):\n",
    "    tweet_text = re.sub(r'&amp;', \"and\", tweet_text)\n",
    "    tweet_text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',\n",
    "                        tweet_text)\n",
    "    #tweet_text = re.sub('(@[A-Za-z0-9\\_]+)', '', tweet_text)\n",
    "    tweet_text = re.sub('(@[\\S]+)', '', tweet_text)#added Angel\n",
    "    ####the following section separate words following each hashtag\n",
    "    list_hashtag1=re.findall('(#[\\S]+)',tweet_text)\n",
    "    list_hashtag2=[]\n",
    "    for p in list_hashtag1:\n",
    "        p=re.sub('(#)',' ',p)#clear hashtag symbols\n",
    "        p1=re.findall('([A-Z]{2,})',p)#uppercase abbrevs\n",
    "        for p2 in p1:\n",
    "            list_hashtag2.append(p2)#add abbrevs\n",
    "        p=re.sub('([A-Z]{2,})',' ',p)#clear uppercase abbrevs\n",
    "        p1=re.findall('([A-Z][a-z]{1,})',p)#words start uppercase letter\n",
    "        for p2 in p1:\n",
    "            list_hashtag2.append(p2)\n",
    "        p=re.sub('([A-Z][a-z]{1,})',' ',p)#clear words start with uppercase letter\n",
    "        p=re.sub('([A-Z])',' ',p)#clear single uppercase letters\n",
    "        p=re.sub('([^A-Za-z])',' ',p)#clear symbols\n",
    "        p1=p.split()#find leftover words\n",
    "        for p2 in p1:\n",
    "            list_hashtag2.append(p2)#add leftover words\n",
    "    tweet_text=re.sub('(#[\\S]+)','',tweet_text)\n",
    "    \n",
    "    tweet_text = re.sub(\"[^a-zA-Z_-]\", \" \", tweet_text)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    tweet_text = re.sub(r'\\s{2,}', \" \", tweet_text)\n",
    "    list_hashtag= [h for h in list_hashtag2 if (not h in stop_words and len(h)>1)]\n",
    "    tweet_text = [t for t in tweet_text.split() if (not t in stop_words and len(t)>1)]\n",
    "    tweet_text.extend(list_hashtag)\n",
    "    return ' '.join(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(loadStopWords('./stopwords.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_remove_stopwords'] = df['unprocessed_text'].apply(lambda x: processing_text_remove_stopwords(x,stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find the most frequent words in text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = ' '.join(df['text_remove_stopwords'].to_list()).split()\n",
    "bigrams = []\n",
    "for tweet in df['text_remove_stopwords'].to_list():\n",
    "    bigrams += list(nltk.bigrams(tweet.split()))\n",
    "    \n",
    "trigrams = []\n",
    "for tweet in df['text_remove_stopwords'].to_list():\n",
    "    trigrams += list(nltk.trigrams(tweet.split()))\n",
    "uni_fd = nltk.FreqDist(unigrams)\n",
    "big_fd = nltk.FreqDist(bigrams)\n",
    "trig_fd = nltk.FreqDist(trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def wordInNumTweets(s):\n",
    "    pattern = rf'(^|[^a-zA-Z]){s}([^a-zA-Z]|$)' #rf is for using a variable inside\n",
    "    count=0\n",
    "    for i in range(len(df['text_remove_stopwords'])):\n",
    "         if re.search(pattern,df['text_remove_stopwords'].iloc[i]) is not None :\n",
    "            count=count+1\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams:\n",
      "\n",
      "people 66954\n",
      "social 64162\n",
      "security 59281\n",
      "care 51287\n",
      "get 48687\n",
      "health 41356\n",
      "would 40135\n",
      "pay 39945\n",
      "insurance 39921\n",
      "trump 39077\n",
      "healthcare 38641\n",
      "expansion 38198\n",
      "like 35822\n",
      "cut 30612\n",
      "tax 29473\n",
      "state 29422\n",
      "cuts 28059\n",
      "need 26424\n",
      "want 26282\n",
      "food 23709\n",
      "work 23698\n",
      "programs 23589\n",
      "us 23326\n",
      "re 23125\n",
      "money 22908\n",
      "states 21239\n",
      "aca 20763\n",
      "free 20572\n",
      "one 19826\n",
      "know 19507\n",
      "also 19277\n",
      "take 19129\n",
      "government 19113\n",
      "poor 19012\n",
      "public 18829\n",
      "many 18774\n",
      "stamps 18513\n",
      "budget 18252\n",
      "coverage 17871\n",
      "make 17382\n",
      "--------------------------------------------------------\n",
      "\n",
      "bigrams:\n",
      "\n",
      "social security 56343\n",
      "food stamps 18233\n",
      "health care 16441\n",
      "health insurance 8651\n",
      "tax cuts 6999\n",
      "private insurance 5939\n",
      "cut social 5701\n",
      "cuts social 4495\n",
      "take away 4110\n",
      "work requirements 3636\n",
      "middle class 3528\n",
      "low income 3129\n",
      "mental health 3037\n",
      "budget cuts 2976\n",
      "pre-existing conditions 2964\n",
      "Medicare Medicaid 2922\n",
      "trump budget 2918\n",
      "For All 2791\n",
      "insurance companies 2751\n",
      "tax cut 2736\n",
      "poor people 2705\n",
      "public schools 2632\n"
     ]
    }
   ],
   "source": [
    "num = 40\n",
    "\n",
    "print('unigrams:\\n')\n",
    "fd = uni_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "df_term1=[]\n",
    "df_count1=[]\n",
    "df_numtweets1=[]\n",
    "for i in range(0,num):\n",
    "    print(fd_list[i][0], fd_list[i][1])\n",
    "    df_term1.append(fd_list[i][0])\n",
    "    df_count1.append(fd_list[i][1])\n",
    "    df_numtweets1.append(wordInNumTweets(fd_list[i][0]))\n",
    "\n",
    "print('--------------------------------------------------------\\n')\n",
    "print('bigrams:\\n')\n",
    "fd = big_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "df_term2=[]\n",
    "df_count2=[]\n",
    "df_numtweets2=[]\n",
    "for i in range(0,num):\n",
    "    x, y= fd_list[i][0]\n",
    "    term = x + ' '+ y\n",
    "    print(term, fd_list[i][1])\n",
    "    df_term2.append(term)\n",
    "    df_count2.append(fd_list[i][1])\n",
    "    df_numtweets2.append(wordInNumTweets(term))\n",
    "\n",
    "print('--------------------------------------------------------\\n')\n",
    "print('trigrams:\\n')\n",
    "fd = trig_fd\n",
    "fd_list = [(x,fd[x]) for x in fd]\n",
    "fd_list.sort(key = lambda x: x[1], reverse = True)\n",
    "df_term3=[]\n",
    "df_count3=[]\n",
    "df_numtweets3=[]\n",
    "for i in range(0,num):\n",
    "    x, y, z= fd_list[i][0]\n",
    "    term = x + ' '+ y + ' ' + z\n",
    "    print(term, fd_list[i][1])\n",
    "    df_term3.append(term)\n",
    "    df_count3.append(fd_list[i][1])\n",
    "    df_numtweets3.append(wordInNumTweets(term))\n",
    "df_ubt={'uni_term':df_term1,'uni_count':df_count1,'uni_numtw':df_numtweets1,'bi_term':df_term2,'bi_count':df_count2,'bi_numtw':df_numtweets2,'tri_term':df_term3,'tri_count':df_count3,'tri_numtw':df_numtweets3}\n",
    "df_ubt=pd.DataFrame(df_ubt)\n",
    "df_ubt.to_csv('ubt40.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add sentiment scores to all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(df['polarity'].iloc[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(df['polarity'].iloc[111])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "print(df['polarity'].iloc[311])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from textblob import TextBlob\n",
    "def addpolarityscores(text):\n",
    "    t=TextBlob(text)\n",
    "    return t.sentiment.polarity\n",
    "def addsubjectivityscores(text):\n",
    "    t=TextBlob(text)\n",
    "    return t.sentiment.subjectivity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "df['polarity'] = df['text_remove_stopwords'].apply(lambda x:addpolarityscores(x))\n",
    "df['subjectivity'] = df['text_remove_stopwords'].apply(lambda x:addsubjectivityscores(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add label of 1/0 based on the existence/absence of each interesting term"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highfreqword(text,terms):\n",
    "    exist=0\n",
    "    for j in range(len(terms)):\n",
    "        pattern = rf'(^|[^a-zA-Z]){terms[j]}([^a-zA-Z]|$)' #rf is for using a variable inside\n",
    "        if(re.search(pattern,text)!=None):\n",
    "            exist=1\n",
    "    if exist==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "term_ls=[['cut social security','cutting social security','cuts social security','social security cuts'],['mental health'],['middle class'],['affordable care act','aca'],['tax cut','tax cuts'],['food stamps'],['low income'],['planned parenthood'],['minimum wage'],['illegal immigrants']]\n",
    "for i in range(10):\n",
    "    label_name=(term_ls[i])[0]\n",
    "    df[label_name] = df['text_remove_stopwords'].apply(lambda x:highfreqword(x,term_ls[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "3382"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp=df[df['middle class'].apply(lambda x:x==1)]\n",
    "len(df_temp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-132-7e03087d1343>:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_temp1=df_temp[df[bigrams_ls[i]].apply(lambda x:x==1)]\n",
      "<ipython-input-132-7e03087d1343>:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_temp1=df_temp[df['healthcare'].apply(lambda x:x==1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862\n",
      "11\n",
      "458\n",
      "385\n",
      "125\n",
      "157\n",
      "100\n",
      "47\n",
      "108\n",
      "75\n",
      "8\n",
      "31\n",
      "43\n",
      "18\n",
      "144\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    df_temp1=df_temp[df[bigrams_ls[i]].apply(lambda x:x==1)]\n",
    "    print(len(df_temp1))\n",
    "df_temp1=df_temp[df['healthcare'].apply(lambda x:x==1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "#term2 = 'trump budget'\n",
    "#pattern = rf'(^|[^a-zA-Z]){term2}([^a-zA-Z]|$)' #rf is for using a variable inside\n",
    "#df_temp2 = df[df['text_remove_stopwords'].apply(lambda x: re.search(pattern,x)!=None)]\n",
    "#print('# of tweets:',len(df_temp2))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of duplicated tweets: 0\n"
     ]
    }
   ],
   "source": [
    "#df_temp12 =df_temp1.append(df_temp2)\n",
    "#print('# of duplicated tweets:',df_temp12.duplicated(subset = ['tweet_id'], keep=False).sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "num = 30\n",
    "temp = df_temp1.sample(n=num)\n",
    "#for i in range(0,num):\n",
    "    #print(temp['unprocessed_text'].iloc[i],'\\n\\n')\n",
    "    #print(temp['text_remove_stopwords'].iloc[i],'\\n\\n')\n",
    "    #print('---------------------------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp.to_csv('ss_pec_sample.csv')\n",
    "#temp.to_csv('mh_e_sample.csv')\n",
    "temp.to_csv('mc_h_sample.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}